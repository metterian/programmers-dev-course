# 선형대수 - 선형변환(Linear Transformation)

## 들어가며

### 매핑룰(mapping rule)

중등 교과과정에서 배운 함수의 개념은 다음과 같다.

> 함수는 두 집합간의 **맵핑룰**(mapping rule)이다. 

입력이 정의되는 D를 **정의역**(domain)이라고 한다. 출력이 정의되는 집합 C를 co-domain(**공역**: 쌍을 이루는)이라고 하여, co-domain 중 실제 함수의 출력이 나오는 부분집합을 range(**치역**)이라고 한다.

함수 f는 아래 그림과 같이 $D$의 각 원소 $x$가 $c$의 한 원소 $y(=f(x))$에 대응되는 **매핑룰**(mapping role)이다.
$$
D \ \xrightarrow{f}\  C \ (혹은) \ f:D \ \rightarrow \ C
$$
<img src="https://tva1.sinaimg.cn/large/008i3skNgy1gq0em5hdsxj30xc0p076i.jpg" alt="Codomain - Wikipedia" width="200"  />

[^출처]: 위키피디아

고등 교과 과정에서 다음과 같은 함수를 자주 접해 봤을 겁니다.
$$
f(x)=x^{2}+2 x+3
$$
이 함수는 domain과 co-domain을 강조하여 다음과 같이 적을 수 있습니다.
$$
\mathbb{R} \ \xrightarrow{f} \ \mathbb{R} \ (혹은)\ \  f: \mathbb{R} \ \rightarrow \ \mathbb{R}
$$
C-스타일의 프로그래밍을 하다보면 다음과 같이 함수 $f$를 구현 할 수 있습니다. 입출력을 실수 집합 $R$(real number)을 다루는 함수 라는 것을 명시하기 위해 입출력 타입을 `float`으로 지정했습니다.

```c
float f(float x) {return x*x + 2*X +3;}
```



<br>

## 선형변환

### 선형변환의 의미

사상은 한 집합의 원소를 다른 집합의 **원소로 대응시키는 것** 즉 일종의 함수를 말한다. 선형변환은 사상에 의해 대응되는 두 집합이 벡터공간인 특별한 사상으로, 다음과 같이 정의합니다.

> 벡터공간 $V$에서 벡터공간 $W$로 가는 사상 $L : V \rightarrow W$ 가 다음 두 조건을 만족하면,
>
> 이를 **선형변환**(inear transformation) 또는 **선형사상**(linear mapping)이라 한다. 
>
> (1) $L(u+v)=L(u)+L(v)$
> (2) $L(c u)=c L(u)$

여기서의 선형변환 혹은 선형 사상의 의미는 **선형함수**(linear fuction)을 의미하기도 합니다. 즉, 이전에는 함수를 직접그래서 선(linear)의 형태를 파악했어야 했습니다. 하지만, 위의 선형변환 정의를 통해 함수를 직접 그리지 않고도 선형의 유무를 파악 할 수 있게 되었습니다. 

  




#### $L(u+v)=L(u)+L(v)$ 의 의미

식을 좀더 잘펴보자면, 좌항에서의 덧셈과 우항에서의 덧셈은 다른 종류의 덧셈입니다. 좌항에서의 덧셈은 domain에서의 덧셈이고, 우항에서의 덧셈은 co-domain에서의 덧셈입니다. 그러므로, 좌항과 우항이 다른게 정상입니다. 하지만, 선형함수, 즉 직선 처럼 생겼으면 신기하게 좌항과 우항의 값이 같게 됩니다. 즉 정리하면 다음과 같습니다.

> **덧셈(+) 연산을 먼저 수행**한 다음, 함수를 수행한 결과와 입력에 대해 함수를 **수행한 후 나온 결과에 대해 덧셈(+) 연산을 수행**한 결과는 같다.

<img src="https://tva1.sinaimg.cn/large/008i3skNgy1gq0ezffu7uj30g40csqai.jpg" alt="image-20210429113943065" width="300"  />

#### $L(c u)=c L(u)$의 의미

위 식도 마찬가지 입니다. 좌항의 결과가 우항의 결과가 달라야 하지만 결과가 같게 됩니다. 정리하면 다음과 같습니다. 

> **스칼라 곱셉 연산을 먼저 수행**한 다음, 함수를 수행한 결과와 입력에 대해 함수를 **수행한 후 나온 결과에 대해 스칼라 곱셈 연산**을 수행한 결과는 같다.



<br>

## 변환(Transformation)

> 함수의 입력이 $n$-벡터이고 출력이 $m$-벡터인 함수 $T$를 생각해 봅시다. 이와같이 함수의 입출력이 벡터인 함수를 **변환(transformation)**이라고 합니다.
> $$
> T : \mathbb{R}^n \ \rightarrow \ \mathbb{R}^m
> $$
> 특별히, $n=m$인 경우, 해당 변환을 **연산자(operator)**라고 합니다.

#### 변환의 예: MNIST 손글씨 인식문제

예를 들어, $28 \times 28$ 손글씨 숫자영상을 그레이스케일로 받아, 0부터 9까지의 어던 숫사가 적혀 있는지 알아내는 MNIST 손글씨 인식 문제는 다음과 같은 (비선형)**변환** 입니다.
$$
T : \mathbb{R}^{28 \times 28} \rightarrow \mathbb{R}^{10}
$$
<br>

#### 행렬변환(Matrix Transformation)

> $m \times n$ 행렬은 $n$-벡터를 입력받아 $m$-벡터를 출력으로 내는 선형 변환 이며, **임의의 선형변환은 행렬로 표현가능** 하다. 즉, **행렬은 선형변환의 구현체** 입니다.

$m \times n$ 행렬은 $A$에 대해 $Ax$는 $n$-벡터를 입력 받아 $m$-벡터를 출력으로 내는 변환 $T_A(x) = Ax$ 로 볼 수있습니다. 이 변환은 행렬이 정의하기 때문에 **행렬변환**(matrix transformation)이라고 합니다.
$$
T_A : \mathbb{R}^n \ \rightarrow \ \mathbb{R}^m
$$
그런데 행렬변환은 다음의 선형함수 성질을 모두 만족하기 때문에 **선형변환**(linear transformation)이다.
$$
\begin{aligned}
T_{A}(\mathbf{x}+\mathbf{y}) &=T_{A}(\mathbf{x})+T_{A}(\mathbf{y}) \\
T_{A}(c \mathbf{x}) &=c T_{A}(\mathbf{x})
\end{aligned}
$$
위 식을 풀이하면 다음과 같다. 좌항의 행렬 변환은 행렬 $A$, $n$-벡터가 존재하는 행렬이다. 즉, $n$-벡터들간의 덧셈이다. 하지만, 우항의 덧셈은 변환이 완료된 $m$-벡터들간의 덧셈이다. 스칼라배도 이와 동일하다,



<br>

## 표준 행렬(standard matrix)

### 행렬변환 코딩하기

> 다음 절차를 통해 우리가 원하는 방식대로 동작하는 행렬변환을 코딩 할 수 있다.
>
> 1. 구현하고자하는 기능(function)의 입력과 출력이 **벡터로 정의**되는지 확인 한다. 
> 2. 구현하고자 하는 기능이 **선형**인지 확인 한다.
> 3. 입력이 $n$-벡터이고, 출력이 $m$-벡터이면 $m \times n$ **표준 행렬**을 구성한다.

<br>

### 표준행렬을 이용한 선형변환 코딩

다음의  $m \times n$ 표준행렬(standard matrix)을 구성 함으로써, 우리가 원하는 방식대로 동작하는 행렬변환 $T_A : \mathbb{R}^n \ \rightarrow \ \mathbb{R}^m$ 을 코딩 할 수 있다.
$$
A=\left[T_{A}\left(\mathbf{e}_{1}\right) \quad T_{A}\left(\mathbf{e}_{2}\right) \quad \cdots \quad T_{A}\left(\mathbf{e}_{n}\right)\right]_{m \times n}
$$

> **표준 행렬 구하기**
>
> - n-차원 표준 기저벡터 ${e_1, e_2, \dots, e_n}$을 생각한다.
> - 각 $n$-차원 표준 기저베겉 $e_i$에 데해, 우리가 원하는 기능을 동작 시켜 얻은 결과인 $m$-차원 벡터 $T(e_i)$를 표준 행렬의 각 열에 적는다.

