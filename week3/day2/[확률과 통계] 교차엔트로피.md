# 엔트포피

## 엔트로피의 개념

### 자기정보 (self-information)

자기 정보는 보통 $i(A)$ 로 표기하고 여기서 A는 사건을 뜻한다. 이를 다음과 같이 정의 한다.
$$
i(A)=\log _{b}\left(\frac{1}{P(A)}\right)=-\log _{b} P(A)
$$

### 확률이 높은 사건

확률이 높은 사건은 **정보가 많지 않다.** 반대로, 확률이 낮은 사건이 정보를 많이 포함 하고 있다. 예를 들어, 도둑이 들었는데 개가 짖는 경우보다 도둑이 들었는데 개가 안 짖는 경우(=확률이 낮은 경우) 더 많은 정보를 포함하고 있음

위의 이러한 조건을 맞족하는 함수를 찾다 보니, $log$를 사용하게 되었다.

### 정보의 단위

$$
\begin{array}{l}
b=2: \text { bits } \\
b=e: \text { nats } \\
b=10: \text { hartleys }
\end{array}
$$



### 특성

두 사건이 동시에 있어 났을 때의 자기 정보는 각각의 자기 정보를 각각 더한 것과 같다. 즉, 동시에 일어난 정보의 양이 각각에 일어난 정보의 양을 합친 것과 같은 것이다. 
$$
\begin{align}
i(A B)&=\log _{b}\left(\frac{1}{P(A) P(B)}\right)\\
&=\log _{b}\left(\frac{1}
{P(A)}\right)+\log _{b}\left(\frac{1}{P(B)}\right)\\
&=i(A)+i(B)
\end{align}
$$
예를 틀어, 동전 던지기를 할때의 확률을 다음과 같이 정의했다고 해보자. $P(H)=\frac{1}{8}, P(T)=\frac{7}{8}$ 이때, head가 나올 확률은 3비트의 정보를 토함하고 있고, tail이 나올 확률의 정보는 0.193 비트를 포함 하고 있다. 이를 수식으로 다음과 같이 표현 할 수 있다.
$$
i(H)=3 \text { 비트, } i(T)=0.193 \text { 비트 }
$$

## 엔트로피 (entropy)

> 엔트로피란 **자기 정보의 평균**을 의미한다. 이는 다음과 같이 수식으로 정리 할 수 있다.
> $$
> H(X)=\sum_{j} P\left(A_{j}\right) i\left(A_{j}\right)=-\sum_{j} P\left(A_{j}\right) \log _{2} P\left(A_{j}\right)
> $$

### 특성

$$
\begin{array}{l}
0 \leq H(X) \leq \log _{2} K, \\
\quad K: \text { 사건의 수 }
\end{array}
$$

### 엔트로피 활용

- 평균 비트수를 표현
- 데이터 압축에 사용 가능

예를 들어, 4가지 문자(정보)를 표현하는데 필요한 비트수를 계산한다고 해보자. 단, 모든 문자가 동일한 확률로 구성 되어 있지 않과 다음과 같은 확류로 구성 되어 있다고 가정 해보자

| $X$  |    $P(X)$     | $i(X)$ | A-code |
| :--: | :-----------: | :----: | :----- |
|  A   | $\frac{1}{2}$ |   1    | 0      |
|  B   | $\frac{1}{4}$ |   2    | 10     |
|  C   | $\frac{1}{8}$ |   3    | 110    |
|  D   | $\frac{1}{8}$ |   3    | 111    |

일반적인 경우, 4가지 정보를 표현하는데 2비트가 필요하다. 하지만, $i(X)$를 활용하는 경우 평균 비트수로 표현이 가능하다. (허프만 코드 등을 이용)
$$
1 \times \frac{1}{2}+2 \times \frac{1}{4}+3 \times \frac{1}{8}+3 \times \frac{1}{8}=\frac{14}{8}=\frac{7}{4} \text { 비트 }
$$
4가지 정보를 2비트를 사용해서 보낼 때보다. 0.25 정도 작게 비트를 전송 할 수 있게 된다.

<br>

# 교차 엔트로피

## 교차 엔트로피의 개념

### 확률 분포 $P$와 $Q$

> 집합 $S$에 여러가지 사건이 있다고 가정 해보자. $S=\left\{A_{j}\right\}$ 이를 다음과 같이 표기가 가능하다. 
> $$
> P\left(A_{j}\right): \text { 확률분포 } P \text { 에서 사건 } A_{j} \text { 가 발생할 확률 }\\
> Q\left(A_{j}\right): \text { 확률분포 } Q \text { 에서 사건 } A_{j} \text { 가 발생할 확률 }\\
> i\left(A_{j}\right): \text { 확률분포 } Q \text { 에서 사건 } A_{j} \text { 의 자기정보 }
> $$

자기 정보는 $A_{j}$를 표현 하는 비트 수 이다. 즉, $i\left(A_{j}\right)=-\log _{2} Q\left(A_{j}\right)$ 로 표현이 가능하다. 잘못된 확률 분포 $Q$를 사용하게 되면, 실제 최적의 비트수를 사용하기 못하게 된다. 그러므로, 한 개의 확률 분포를 사용하지 않고 두 개의 확률 분포를 사용해서 교차 엔트로피를 정의 하게 된다.

<br>

### H(P, Q) = 교차 엔트로피

> $H(P, Q)$는 집합 $S$상에서 확률 분포 $P$에 대한 확률 분포 $Q$의 교차 엔트로피를 의미한다. 

#### 확률분포 P에서 $i\left(A_{j}\right)$ 의 평균

$$
\begin{align}
H(P, Q)&=\Sigma_{j} P\left(A_{j}\right) i\left(A_{j}\right)\\
&=-\sum_{j} P\left(A_{j}\right) \log _{2} \boldsymbol{Q}\left(\boldsymbol{A}_{j}\right)\\
&=-\sum_{x \in \mathrm{X}} P(x) \log _{2} \boldsymbol{Q}(\boldsymbol{x})
\end{align}
$$

이 값은 정확한 확률 분포 P를 사용했을 때의 비트수 보다 크게된다.
$$
H(P, Q)=-\sum_{x \in \mathrm{X}} P(x) \log _{2} Q(x) \geq-\sum_{x \in \mathrm{X}} P(x) \log _{2} P(x)=H(P)
$$
따라서, 이 값은 **P와 Q가 얼마나 비슷한지**를 표현하는 것이다.
$$
\begin{array}{l}
\text { 같으면 } H(P, Q)=H(P) \\
\text { 다르면 } H(P, Q)>H(P)
\end{array}
$$
<br>

### 예제

실제 각 문자가 등장하는 확률을 $P(X)$라 하고 우리가 구한 확률을 $Q(X)$라고 해보자. 새로운 확률 $Q(X)$를 통해 새로운 자기정보를 얻을 수 있게 될 것이다. 

| $X$  |    $P(X)$     | $i(X)$ |     Q(X)      | $i(X)$ | Q-code  |
| :--: | :-----------: | :----: | :-----------: | :----: | ------- |
|  A   | $\frac{1}{2}$ |   1    | $\frac{1}{8}$ | **3**  | **000** |
|  B   | $\frac{1}{4}$ |   2    | $\frac{1}{8}$ | **3**  | **001** |
|  C   | $\frac{1}{8}$ |   3    | $\frac{1}{4}$ | **2**  | **01**  |
|  D   | $\frac{1}{8}$ |   3    | $\frac{1}{2}$ | **1**  | **1**   |

새로운 분포 $Q(X)$를 통해 얻은 자기정보 $i(X)$를 통해 실제의 분포 $P(X)$에 대한 평균 비트 수를 구하면 다음과 같다.
$$
3 \times \frac{1}{2}+3 \times \frac{1}{4}+2 \times \frac{1}{8}+1 \times \frac{1}{8}=\frac{21}{8} \text { 비트 }
$$
즉, 기존 보다 1.5배 더 많은 비트 수를 사용한다. 



<br>

## 분류 문제에서의 손실 함수

### 분류 문제

- 주어진 대상이 $A$ 인지 아닌지를 판단
- 주어진 대상이 $A, B, C, \ldots$ 중 어느 것인지를 판단

기계학습에서는 **주어진 대상이 각 그룹에 속할 확률**을 제공 해준다. 예를 들어, [0.8,0.2]: A$ 일 확률 $0.8$, 아닐 확률 $0.2​로 주어 졌다고 가정해보자. 이때 이 값의 <u>정답인 [1.0, 0.0]과 얼마나 다른지</u>가 측정이 가능 해진다.

<br>

### 분류문제에서의 손실 함수

$$
\begin{array}{l}
\text { 원하는 답 } P=\left[p_{1}, p_{2}, \ldots, p_{n}\right], p_{1}+p_{2}+\ldots+p_{n}=1 \\
\text { 제시된 답 } Q=\left[q_{1}, q_{2}, \ldots, q_{n}\right], q_{1}+q_{2},+\cdots+q_{n}=1
\end{array}
$$

$\rightarrow$ $P$ 와 $Q$가 얼마나 다른지에 대한 척도가 필요하다.

#### 제곱합

P와 Q가 얼마나 다른지를 측정하기 위해서 제곱합을 사용한다고 해보자
$$
\sum\left(p_{i}-q_{i}\right)^{2}
$$
위와 같은 식으로 나타낼 수 있고, $(1.0-0.8)^{2}+(0.0-0.2)^{2}$ 으로 계산 되어 진다. 이 때 다음과 같은 특징을 지닌다.

- 확률이 다를수록 큰 값을 지닌다
- 학습 속도가 느리다

#### 교차 엔트로피 $H(P, Q)$

- 확률이 다를수록 큰 값을 가진다
- 학습 속도가 빠르다
- 분류문제에서 주로 교차 엔트로피를 사용한다.

### 참고

$$
P=\left[p_{1}, p_{2}, \ldots, p_{n}\right]
$$

분류 문제에서 우리가 원하는 답은 보통 $p_{i}$ 중 하나만 1이고, 나머지는 다 0이다. 즉, 엔트로피 자체는 0이다. 

이때, $p_{k}=1.0$ 이라고 하면, $q_{k}$ 값이 최대한 커지는 방향으로 학습을 진행한다.



### 예제

#### 실제 상황

$$
\begin{array}{l}
P=[1,0] \\
P(A)=1, P(B)=0
\end{array}
$$

위 식과 같이 실제 확률 분포가 주어 졌다. 이 때 각 예측에 따른 엔트로피를 구하면 다음과 같다.

#### 예측 $Q(x)$

$[0.8,0.2]: Q(A)=0.8, Q(B)=0.2$ 인 경우
$$
H(P, Q)=-\sum_{x \in \mathrm{X}} P(x) \log _{2} Q(x)=-1 \times \log _{2} 0.8=0.3219
$$
$[0.5,0.5]: Q(A)=0.5, Q(B)=0.5$ 인 경우
$$
H(P, Q)=-\sum_{x \in \mathrm{X}} P(x) \log _{2} Q(x)=-1 \times \log _{2} 0.5=1
$$
$[0.2,0.8]: Q(A)=0.2, Q(B)=0.8$ 인 경우
$$
H(P, Q)=-\sum_{x \in \mathrm{X}} P(x) \log _{2} Q(x)=-1 \times \log _{2} 0.2=2.32
$$

#### 코드 구현

```python
>>> def cross_entropy(P,Q):
...     return sum([-P[i]*np.log2(Q[i]) for i in range(len(P))])
>>> P = [1,0]
>>> Q = [0.8, 0.2]
>>> cross_entropy(P,Q)
0.3219280948873623
```

